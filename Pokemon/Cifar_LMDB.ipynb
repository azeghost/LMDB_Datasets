{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar_LMDB.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZYE3xCyZ6Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/kkahloots/Generative_Models.git # this is for loading git with correct brach\n",
        "%cd /content/Generative_Models/\n",
        "!git checkout lmdb\n",
        "\n",
        "!pip -q install -r /content/Generative_Models/requirements.txt\n",
        "%cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRHuradPaY6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install lmdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxH8QbBwbwDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -P /content --no-check-certificate\n",
        "!mkdir  /content/data\n",
        "!tar -xf /content/cifar-10-python.tar.gz -C /content/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8QzsYATbvHe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "05eb521a-a255-4a40-cda7-f62e4c346312"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Path to the unzipped CIFAR data\n",
        "data_dir = Path(\"/content/data/cifar-10-batches-py/\")\n",
        "\n",
        "# Unpickle function provided by the CIFAR hosts\n",
        "def unpickle(file):\n",
        "    with open(file, \"rb\") as fo:\n",
        "        dict = pickle.load(fo, encoding=\"bytes\")\n",
        "    return dict\n",
        "\n",
        "images, labels = [], []\n",
        "for batch in data_dir.glob(\"data_batch_*\"):\n",
        "    batch_data = unpickle(batch)\n",
        "    for i, flat_im in enumerate(batch_data[b\"data\"]):\n",
        "        im_channels = []\n",
        "        # Each image is flattened, with channels in order of R, G, B\n",
        "        for j in range(3):\n",
        "            im_channels.append(\n",
        "                flat_im[j * 1024 : (j + 1) * 1024].reshape((32, 32))\n",
        "            )\n",
        "        # Reconstruct the original image\n",
        "        images.append(np.dstack((im_channels)))\n",
        "        # Save the label\n",
        "        labels.append(batch_data[b\"labels\"][i])\n",
        "\n",
        "print(\"Loaded CIFAR-10 training set:\")\n",
        "print(f\" - np.shape(images)     {np.shape(images)}\")\n",
        "print(f\" - np.shape(labels)     {np.shape(labels)}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded CIFAR-10 training set:\n",
            " - np.shape(images)     (50000, 32, 32, 3)\n",
            " - np.shape(labels)     (50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQIOinnFaZwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# disk_dir = Path(\"data/disk/\")\n",
        "lmdb_dir = Path(\"./data/lmdb/\")\n",
        "# hdf5_dir = Path(\"data/hdf5/\")\n",
        "\n",
        "# disk_dir.mkdir(parents=True, exist_ok=True)\n",
        "lmdb_dir.mkdir(parents=True, exist_ok=True)\n",
        "# hdf5_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j_VzyY_ah0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lmdb\n",
        "import pickle\n",
        "\n",
        "def store_single_lmdb(image, image_id, label):\n",
        "    \"\"\" Stores a single image to a LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        image_id    integer unique ID for image\n",
        "        label       image label\n",
        "    \"\"\"\n",
        "    map_size = image.nbytes * 10\n",
        "\n",
        "    # Create a new LMDB environment\n",
        "    env = lmdb.open(str(lmdb_dir / f\"single_lmdb\"), map_size=map_size)\n",
        "\n",
        "    # Start a new write transaction\n",
        "    with env.begin(write=True) as txn:\n",
        "        # All key-value pairs need to be strings\n",
        "        value = CIFAR_Image(image, label)\n",
        "        key = f\"{image_id:08}\"\n",
        "        txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eggEE7ySaEPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CIFAR_Image:\n",
        "    def __init__(self, image, label):\n",
        "        # Dimensions of image for reconstruction - not really necessary \n",
        "        # for this dataset, but some datasets may include images of \n",
        "        # varying sizes\n",
        "        self.channels = image.shape[2]\n",
        "        self.size = image.shape[:2]\n",
        "\n",
        "        self.image = image.tobytes()\n",
        "        self.label = label\n",
        "\n",
        "    def get_image(self):\n",
        "        \"\"\" Returns the image as a numpy array. \"\"\"\n",
        "        image = np.frombuffer(self.image, dtype=np.uint8)\n",
        "        return image.reshape(*self.size, self.channels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhkSkgJLap3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store_many_lmdb(images, labels):\n",
        "    \"\"\" Stores an array of images to LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        images       images array, (N, 32, 32, 3) to be stored\n",
        "        labels       labels array, (N, 1) to be stored\n",
        "    \"\"\"\n",
        "    num_images = len(images)\n",
        "\n",
        "    map_size = num_images * images[0].nbytes * 10\n",
        "\n",
        "    # Create a new LMDB DB for all the images\n",
        "    env = lmdb.open(str(lmdb_dir / f\"{num_images}_lmdb\"), map_size=map_size)\n",
        "\n",
        "    # Same as before â€” but let's write all the images in a single transaction\n",
        "    with env.begin(write=True) as txn:\n",
        "        for i in range(num_images):\n",
        "            # All key-value pairs need to be Strings\n",
        "            value = CIFAR_Image(images[i], labels[i])\n",
        "            key = f\"{i:08}\"\n",
        "            txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-KUX3YPasmr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5771c3fa-1b3f-4dd5-865c-8566fad7bfd7"
      },
      "source": [
        "cutoffs = [10, 100, 1000, 10000, 100000]\n",
        "\n",
        "# Let's double our images so that we have 100,000\n",
        "images = np.concatenate((images, images), axis=0)\n",
        "labels = np.concatenate((labels, labels), axis=0)\n",
        "\n",
        "# Make sure you actually have 100,000 images and labels\n",
        "print(np.shape(images))\n",
        "print(np.shape(labels))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 32, 32, 3)\n",
            "(100000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9qwzd2iazZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_single_lmdb(image_id):\n",
        "    \"\"\" Stores a single image to LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        image_id    integer unique ID for image\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        image       image array, (32, 32, 3) to be stored\n",
        "        label       associated meta data, int label\n",
        "    \"\"\"\n",
        "    # Open the LMDB environment\n",
        "    env = lmdb.open(str(lmdb_dir / f\"single_lmdb\"), readonly=True)\n",
        "\n",
        "    # Start a new read transaction\n",
        "    with env.begin() as txn:\n",
        "        # Encode the key the same way as we stored it\n",
        "        data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
        "        # Remember it's a CIFAR_Image object that is loaded\n",
        "        cifar_image = pickle.loads(data)\n",
        "        # Retrieve the relevant bits\n",
        "        image = cifar_image.get_image()\n",
        "        label = cifar_image.label\n",
        "    env.close()\n",
        "\n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMbMlSO0a5ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def read_many_lmdb(num_images):\n",
        "    \"\"\" Reads image from LMDB.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        num_images   number of images to read\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        images      images array, (N, 32, 32, 3) to be stored\n",
        "        labels      associated meta data, int label (N, 1)\n",
        "    \"\"\"\n",
        "    images, labels = [], []\n",
        "    env = lmdb.open(str(lmdb_dir / f\"{num_images}_lmdb\"), readonly=True)\n",
        "\n",
        "    # Start a new read transaction\n",
        "    with env.begin() as txn:\n",
        "        # Read all images in one single transaction, with one lock\n",
        "        # We could split this up into multiple transactions if needed\n",
        "        for image_id in range(num_images):\n",
        "            data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
        "            # Remember that it's a CIFAR_Image object \n",
        "            # that is stored as the value\n",
        "            cifar_image = pickle.loads(data)\n",
        "            # Retrieve the relevant bits\n",
        "            images.append(cifar_image.get_image())\n",
        "            labels.append(cifar_image.label)\n",
        "    env.close()\n",
        "    return images, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9gNobvybW9R",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow has a built-in class LMDBDataset that provides an interface for reading in input data from an LMDB file and can produce iterators and tensors in batches. TensorFlow does not have a built-in class for HDF5, but one can be written that inherits from the Dataset class. I personally use a custom class altogether that is designed for optimal read access based on the way I structure my HDF5 files.\n",
        "\n",
        "More info on https://realpython.com/storing-images-in-python/"
      ]
    }
  ]
}