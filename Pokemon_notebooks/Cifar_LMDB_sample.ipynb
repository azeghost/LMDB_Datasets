{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZYE3xCyZ6Go"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/kkahloots/Generative_Models.git # this is for loading git with correct brach\n",
    "%cd /content/Generative_Models/\n",
    "!git checkout lmdb\n",
    "\n",
    "!pip -q install -r /content/Generative_Models/requirements.txt\n",
    "%cd /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRHuradPaY6h"
   },
   "outputs": [],
   "source": [
    "!pip install lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cxH8QbBwbwDa"
   },
   "outputs": [],
   "source": [
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -P /content --no-check-certificate\n",
    "!mkdir  /content/data\n",
    "!tar -xf /content/cifar-10-python.tar.gz -C /content/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Z8QzsYATbvHe",
    "outputId": "05eb521a-a255-4a40-cda7-f62e4c346312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CIFAR-10 training set:\n",
      " - np.shape(images)     (50000, 32, 32, 3)\n",
      " - np.shape(labels)     (50000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the unzipped CIFAR data\n",
    "data_dir = Path(\"/content/data/cifar-10-batches-py/\")\n",
    "\n",
    "# Unpickle function provided by the CIFAR hosts\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "images, labels = [], []\n",
    "for batch in data_dir.glob(\"data_batch_*\"):\n",
    "    batch_data = unpickle(batch)\n",
    "    for i, flat_im in enumerate(batch_data[b\"data\"]):\n",
    "        im_channels = []\n",
    "        # Each image is flattened, with channels in order of R, G, B\n",
    "        for j in range(3):\n",
    "            im_channels.append(\n",
    "                flat_im[j * 1024 : (j + 1) * 1024].reshape((32, 32))\n",
    "            )\n",
    "        # Reconstruct the original image\n",
    "        images.append(np.dstack((im_channels)))\n",
    "        # Save the label\n",
    "        labels.append(batch_data[b\"labels\"][i])\n",
    "\n",
    "print(\"Loaded CIFAR-10 training set:\")\n",
    "print(f\" - np.shape(images)     {np.shape(images)}\")\n",
    "print(f\" - np.shape(labels)     {np.shape(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQIOinnFaZwq"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# disk_dir = Path(\"data/disk/\")\n",
    "lmdb_dir = Path(\"./data/lmdb/\")\n",
    "# hdf5_dir = Path(\"data/hdf5/\")\n",
    "\n",
    "# disk_dir.mkdir(parents=True, exist_ok=True)\n",
    "lmdb_dir.mkdir(parents=True, exist_ok=True)\n",
    "# hdf5_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4j_VzyY_ah0u"
   },
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import pickle\n",
    "\n",
    "def store_single_lmdb(image, image_id, label):\n",
    "    \"\"\" Stores a single image to a LMDB.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        image       image array, (32, 32, 3) to be stored\n",
    "        image_id    integer unique ID for image\n",
    "        label       image label\n",
    "    \"\"\"\n",
    "    map_size = image.nbytes * 10\n",
    "\n",
    "    # Create a new LMDB environment\n",
    "    env = lmdb.open(str(lmdb_dir / f\"single_lmdb\"), map_size=map_size)\n",
    "\n",
    "    # Start a new write transaction\n",
    "    with env.begin(write=True) as txn:\n",
    "        # All key-value pairs need to be strings\n",
    "        value = CIFAR_Image(image, label)\n",
    "        key = f\"{image_id:08}\"\n",
    "        txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eggEE7ySaEPZ"
   },
   "outputs": [],
   "source": [
    "class CIFAR_Image:\n",
    "    def __init__(self, image, label):\n",
    "        # Dimensions of image for reconstruction - not really necessary \n",
    "        # for this dataset, but some datasets may include images of \n",
    "        # varying sizes\n",
    "        self.channels = image.shape[2]\n",
    "        self.size = image.shape[:2]\n",
    "\n",
    "        self.image = image.tobytes()\n",
    "        self.label = label\n",
    "\n",
    "    def get_image(self):\n",
    "        \"\"\" Returns the image as a numpy array. \"\"\"\n",
    "        image = np.frombuffer(self.image, dtype=np.uint8)\n",
    "        return image.reshape(*self.size, self.channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YhkSkgJLap3i"
   },
   "outputs": [],
   "source": [
    "def store_many_lmdb(images, labels):\n",
    "    \"\"\" Stores an array of images to LMDB.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        images       images array, (N, 32, 32, 3) to be stored\n",
    "        labels       labels array, (N, 1) to be stored\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "\n",
    "    map_size = num_images * images[0].nbytes * 10\n",
    "\n",
    "    # Create a new LMDB DB for all the images\n",
    "    env = lmdb.open(str(lmdb_dir / f\"{num_images}_lmdb\"), map_size=map_size)\n",
    "\n",
    "    # Same as before â€” but let's write all the images in a single transaction\n",
    "    with env.begin(write=True) as txn:\n",
    "        for i in range(num_images):\n",
    "            # All key-value pairs need to be Strings\n",
    "            value = CIFAR_Image(images[i], labels[i])\n",
    "            key = f\"{i:08}\"\n",
    "            txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "q-KUX3YPasmr",
    "outputId": "5771c3fa-1b3f-4dd5-865c-8566fad7bfd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 32, 32, 3)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "cutoffs = [10, 100, 1000, 10000, 100000]\n",
    "\n",
    "# Let's double our images so that we have 100,000\n",
    "images = np.concatenate((images, images), axis=0)\n",
    "labels = np.concatenate((labels, labels), axis=0)\n",
    "\n",
    "# Make sure you actually have 100,000 images and labels\n",
    "print(np.shape(images))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9qwzd2iazZN"
   },
   "outputs": [],
   "source": [
    "def read_single_lmdb(image_id):\n",
    "    \"\"\" Stores a single image to LMDB.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        image_id    integer unique ID for image\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        image       image array, (32, 32, 3) to be stored\n",
    "        label       associated meta data, int label\n",
    "    \"\"\"\n",
    "    # Open the LMDB environment\n",
    "    env = lmdb.open(str(lmdb_dir / f\"single_lmdb\"), readonly=True)\n",
    "\n",
    "    # Start a new read transaction\n",
    "    with env.begin() as txn:\n",
    "        # Encode the key the same way as we stored it\n",
    "        data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
    "        # Remember it's a CIFAR_Image object that is loaded\n",
    "        cifar_image = pickle.loads(data)\n",
    "        # Retrieve the relevant bits\n",
    "        image = cifar_image.get_image()\n",
    "        label = cifar_image.label\n",
    "    env.close()\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YMbMlSO0a5ru"
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_many_lmdb(num_images):\n",
    "    \"\"\" Reads image from LMDB.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        num_images   number of images to read\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        images      images array, (N, 32, 32, 3) to be stored\n",
    "        labels      associated meta data, int label (N, 1)\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "    env = lmdb.open(str(lmdb_dir / f\"{num_images}_lmdb\"), readonly=True)\n",
    "\n",
    "    # Start a new read transaction\n",
    "    with env.begin() as txn:\n",
    "        # Read all images in one single transaction, with one lock\n",
    "        # We could split this up into multiple transactions if needed\n",
    "        for image_id in range(num_images):\n",
    "            data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
    "            # Remember that it's a CIFAR_Image object \n",
    "            # that is stored as the value\n",
    "            cifar_image = pickle.loads(data)\n",
    "            # Retrieve the relevant bits\n",
    "            images.append(cifar_image.get_image())\n",
    "            labels.append(cifar_image.label)\n",
    "    env.close()\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F9gNobvybW9R"
   },
   "source": [
    "TensorFlow has a built-in class LMDBDataset that provides an interface for reading in input data from an LMDB file and can produce iterators and tensors in batches. TensorFlow does not have a built-in class for HDF5, but one can be written that inherits from the Dataset class. I personally use a custom class altogether that is designed for optimal read access based on the way I structure my HDF5 files.\n",
    "\n",
    "More info on https://realpython.com/storing-images-in-python/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Cifar_LMDB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
