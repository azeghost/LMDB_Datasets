{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import lmdb\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.chdir('../') # adress to git dir\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz -O /home/azeghost/git/lmdb_new/LMDB_Datasets/.data/mnist.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data(path='/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "ds = tfds.load('mnist', split='train', shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prep folders for LMDB store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep folders for LMDB store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir .data/MNIST_LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf .data/MNIST_LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls .data/MNIST_LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform from .py calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformation.lmdb_transformer import LmdbTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_percentage = 30\n",
    "valid_format = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numpy_transformer = LmdbTransformer( validation_percentage, valid_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_transformer.transform_store_from_numpy(images =X_test,   labels=Y_test,\n",
    "               lmdb_dir='.data/MNIST_LMDB', category='validation', target_size=None,\n",
    "               color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numpy_transformer.transform_store_from_numpy(images =X_train,  labels=Y_train,\n",
    "               lmdb_dir='.data/MNIST_LMDB', category='training', target_size=None,\n",
    "               color_mode='rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Read the lmdb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Todo\n",
    " - Env put outside of loops for write and read\n",
    " - Try the Dynamic code again with clean kernel \n",
    " - Create Standartize as an option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/azeghost/git/lmdb_new/LMDB_Datasets/.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -la ./MNIST_LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !rm -rf /home/azeghost/git/lmdb_new/LMDB_Datasets/.data/MNIST_LMDB/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets_customed.label_by_filename import read_lmdb\n",
    "ds = read_lmdb('/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/MNIST_LMDB/_training', 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets_customed.label_by_filename import read_lmdb\n",
    "ds_test = read_lmdb('/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/MNIST_LMDB/_validation', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tar and unzip LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/azeghost/git/lmdb_new/LMDB_Datasets/.data')\n",
    "# !ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tar -cvjf mnist.tar.bz MNIST_LMDB/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv mnist.tar.bz /home/azeghost/git/lmdb_new/LMDB_Datasets/.data/MNIST_TAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/azeghost/git/lmdb_new/LMDB_Datasets/.data/MNIST_TAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !split -b 50M mnist.tar.bz \"mnist.tar.part\"\n",
    "#split -b <max size> <name of zip or dir to zip/name> <split file name beginning>\n",
    "\n",
    "#Check if they are created\n",
    "# !ls -lh mnist.tar.part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine Them\n",
    "\n",
    "#pokemon_training\n",
    "# !cat mnist.tar.part* > mnist_combined.tar.bz\n",
    "#!cat <split files put * at the end> > <final zip name>\n",
    "# !ls -la \n",
    "\n",
    "!mkdir ./.test_Mnist\n",
    "\n",
    "!tar -xf mnist.tar.bz --directory ./.test_Mnist\n",
    "\n",
    "# !rm -rf ./.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ./.test_Mnist/MNIST_LMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMDBImageIterator and LMDBImageGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/azeghost/git/Generative_Models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import Iterator, load_img, img_to_array, array_to_img\n",
    "from keras import backend as K\n",
    "import logging\n",
    "from utils.reporting.logging import log_message\n",
    "from utils.data_and_files.file_utils import get_file_path\n",
    "\n",
    "class LMDBImageIterator(Iterator):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_images,\n",
    "                 category,\n",
    "                 lmdb_dir,\n",
    "                 batch_size,\n",
    "                 episode_len=20,\n",
    "                 episode_shift=10,\n",
    "                 shuffle=True,\n",
    "                 seed=None,\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='jpeg',\n",
    "                 dtype=K.floatx(),\n",
    "                 ):\n",
    "        \n",
    "        self.category = category\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        self.lmdb_dir = lmdb_dir\n",
    "        self.episode_len = episode_len\n",
    "        self.episode_shift = episode_shift\n",
    "\n",
    "        \n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "        print(\"Initializing Iterator \" + category +\" Number of images \" +str(num_images))\n",
    "        print(category,lmdb_dir, batch_size,shuffle,seed)\n",
    "        self.env = lmdb.open(lmdb_dir, readonly=True)\n",
    "        \n",
    "        Iterator.__init__(self, num_images, batch_size, shuffle, seed)\n",
    "        \n",
    "        \n",
    "    def __del__(self):\n",
    "        self.env.close()\n",
    "        \n",
    "        \n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        print(index_array)\n",
    "        images, labels = [], []\n",
    "        \n",
    "        if len(index_array) < self.batch_size:\n",
    "            diff = self.batch_size//len(index_array) + 1\n",
    "            index_array = np.repeat(index_array, diff, axis=0)[:self.batch_size]\n",
    "\n",
    "        else:\n",
    "                with self.env.begin() as txn:\n",
    "                    for image_id in index_array:\n",
    "                        data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
    "                        dataset = pickle.loads(data)\n",
    "                        images.append(dataset.get_image())\n",
    "                        labels_list = [attr for attr in dir(dataset) if not callable(getattr(dataset, attr)) and (not attr.startswith(\"__\")) and \n",
    "                           (not attr in ['image','channels',  'size'] )]\n",
    "\n",
    "                        for label in labels_list:\n",
    "                            _lab = {label: eval(f'dataset.{label}')}\n",
    "                            labels = {**labels, **_lab}\n",
    "        return {'images': images, **labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from utils.data_and_files.data_utils import as_bytes\n",
    "from utils.reporting.logging import log_message\n",
    "\n",
    "\n",
    "class LMDBImageGenerator(ImageDataGenerator):\n",
    "    def flow_from_lmdb_lists(self, \n",
    "                              num_images,\n",
    "                              category,\n",
    "                              lmdb_dir,\n",
    "                              batch_size,\n",
    "                              episode_len=None,\n",
    "                              episode_shift=None,\n",
    "                              color_mode='rgb',\n",
    "                              shuffle =True,\n",
    "                              seed=None\n",
    "                              ):\n",
    "\n",
    "        \n",
    "          \n",
    "\n",
    "        return LMDBImageIterator(\n",
    "                             num_images = num_images,\n",
    "                             category = category,\n",
    "                             lmdb_dir = lmdb_dir,\n",
    "                             batch_size  = batch_size,\n",
    "                             episode_len = episode_len,\n",
    "                             episode_shift =episode_shift,\n",
    "                             shuffle = shuffle,\n",
    "                             seed = seed)\n",
    "\n",
    "\n",
    "def get_generators( val_lmdb_dir, val_num_images, tra_lmdb_dir, tra_num_images, \n",
    "                   batch_size, episode_len=None, episode_shift=None):\n",
    "\n",
    "    train_datagen = LMDBImageGenerator()\n",
    "\n",
    "    valid_datagen = LMDBImageGenerator()\n",
    "\n",
    "    train_generator = train_datagen.flow_from_lmdb_lists(\n",
    "        num_images = tra_num_images,\n",
    "        category='training',\n",
    "        lmdb_dir=tra_lmdb_dir,\n",
    "        batch_size=batch_size,\n",
    "        episode_len=episode_len,\n",
    "        episode_shift=episode_shift,\n",
    "        seed=0)\n",
    "\n",
    "    validation_generator = valid_datagen.flow_from_lmdb_lists(\n",
    "        num_images = val_num_images,\n",
    "        category='validation',\n",
    "        lmdb_dir=val_lmdb_dir,\n",
    "        batch_size=batch_size,\n",
    "        episode_len=episode_len,\n",
    "        episode_shift=episode_shift,\n",
    "        seed=0)\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator, testing_generator = get_generators(\n",
    "    val_lmdb_dir = '/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/Pokemon_LMDB/_validation',\n",
    "    val_num_images = 218,\n",
    "    tra_lmdb_dir = '/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/Pokemon_LMDB/_training', \n",
    "    tra_num_images = 591,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y = training_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: training_generator, \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: training_generator, \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: training_generator, \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: testing_generator,     \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")\n",
    "\n",
    "_instance_scale=1.0\n",
    "for data in train_ds:\n",
    "    _instance_scale = float(data[0].numpy().max())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}