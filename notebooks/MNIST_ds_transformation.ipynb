{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import lmdb\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azeghost/git/lmdb_new/LMDB_Datasets\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../') # adress to git dir\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import Iterator, load_img, img_to_array, array_to_img\n",
    "# from transformation.file_utils import get_file_path\n",
    "# from transformation.file_image_generator import create_image_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformation.lmdb_transformer import LmdbTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store images in lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf storage.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz -O /home/azeghost/git/lmdb_new/LMDB_Datasets/.data/mnist.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -la ./.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data(path='/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LmdbTransformer:\n",
    "    def __init__(self,image_dir = None, validation_pct = 30, valid_image_formats = ['.png'],\n",
    "                 data_format= None, scalar = 255.0, *args, **kwargs):\n",
    "        #args -- tuple of anonymous arguments\n",
    "        #kwargs -- dictionary of named arguments\n",
    "#         self.image_dir = kwargs.get('image_dir',)\n",
    "#         self.validation_pct = kwargs.get('validation_pct',) \n",
    "#         self.valid_image_formats = kwargs.get('valid_image_formats',) \n",
    "#         self.data_format= None = kwargs.get('data_format',)\n",
    "#         self.scalar = 255.0 = kwargs.get('scalar',)\n",
    "        \n",
    "\n",
    "        if not image_dir is None:\n",
    "            self.image_lists =create_image_lists(image_dir, validation_pct, valid_image_formats)\n",
    "        else:\n",
    "            self.images = kwargs.get('images')\n",
    "            self.labels = kwargs.get('labels')\n",
    "        self.scaler = scalar \n",
    "        if data_format is None:\n",
    "           self.data_format = K.image_data_format()\n",
    "        else:\n",
    "           self.data_format = data_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Wrapper class for dataset\n",
    "class DatasetWrapper:\n",
    "    def __init__(self, image, labels_dict):\n",
    "        try:\n",
    "            self.channels = image.shape[2]\n",
    "        except:\n",
    "            self.channels = 1\n",
    "        self.size = image.shape[:2]\n",
    "        self.image = image.tobytes()\n",
    "#         for k, val in labels_dict.items():\n",
    "#             exec(f'self.{k}={val}')\n",
    "        self.label = labels_dict #additional data to be stored (make it string)\n",
    "\n",
    "    def get_image(self):\n",
    "        \"\"\" Returns the image as a numpy array. \"\"\"\n",
    "        images = np.frombuffer(self.image, dtype=np.float32) #pay attention if you  don't use create_image_lists\n",
    "        return images.reshape(*self.size, self.channels)     #then dtype will be different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_single_lmdb(filename, img, index, labels_dict, num_images):\n",
    "    \"\"\" Stores a wrapper to LMDB.\n",
    "    \"\"\"\n",
    "    map_size = num_images * img.nbytes * 10\n",
    "    env = lmdb.open(filename, map_size=map_size)\n",
    "\n",
    "    # Same as before — but let's write all the images in a single transaction\n",
    "    with env.begin(write=True) as txn:\n",
    "        # All key-value pairs need to be Strings\n",
    "        value = DatasetWrapper(img, labels_dict)\n",
    "        key = f\"{index:08}\"\n",
    "        txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_store_from_numpy(images, labels,\n",
    "               lmdb_dir='.data/', category='training', target_size=None,\n",
    "               color_mode='rgb'):\n",
    "    create_if_not_exist(lmdb_dir)\n",
    "\n",
    "    classes = list(set(labels))\n",
    "    num_class = len(classes)\n",
    "#     class2id = dict(zip(classes, range(len(classes))))\n",
    "#     id2class = dict((v, k) for k, v in class2id.items())\n",
    "\n",
    "#     for label_name in classes:\n",
    "#         filter_arr = labels == label_name\n",
    "        \n",
    "#         fil_images = images[filter_arr]\n",
    "#         fil_labels = labels[filter_arr]\n",
    "#         num_images_per_label = fil_labels.shape[0]\n",
    "        \n",
    "#         print('Storing ' + str(num_images_per_label) + ' of label {}'.format(str(label_name)) +\n",
    "#               ' In ' +lmdb_dir + os.sep + '_{}_{}'.format(category, str(label_name))\n",
    "    index = 0\n",
    "    for image, label in zip(fil_images, fil_labels):\n",
    "#             / self.scaler\n",
    "        img = np.float32(image)\n",
    "        lmdb_name = lmdb_dir + os.sep + '_{}_{}'.format(category, str(label_name))\n",
    "        store_single_lmdb(index=index, filename=lmdb_name, img=img, labels_dict=label, num_images=num_images_per_label)\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_store_from_numpy(images =X_train,  labels=Y_train,\n",
    "               lmdb_dir='.data/MNIST_LMDB', category='training', target_size=None,\n",
    "               color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir .data/MNIST_LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf .data/MNIST_LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls .data/MNIST_LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform from .py calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformation.lmdb_transformer import LmdbTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_percentage = 30\n",
    "valid_format = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numpy_transformer = LmdbTransformer( validation_percentage, valid_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/10000 [00:00<01:22, 121.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 10000.data/MNIST_LMDB _training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:41<00:00, 240.93it/s]\n"
     ]
    }
   ],
   "source": [
    "numpy_transformer.transform_store_from_numpy(images =X_test,   labels=Y_test,\n",
    "               lmdb_dir='.data/MNIST_LMDB', category='training', target_size=None,\n",
    "               color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 41/60000 [00:00<02:26, 408.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 60000.data/MNIST_LMDB _validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [02:35<00:00, 384.88it/s]\n"
     ]
    }
   ],
   "source": [
    "numpy_transformer.transform_store_from_numpy(images =X_train,  labels=Y_train,\n",
    "               lmdb_dir='.data/MNIST_LMDB', category='validation', target_size=None,\n",
    "               color_mode='rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the lmdb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Todo\n",
    " - Env put outside of loops for write and read\n",
    " - Try the Dynamic code again with clean kernel \n",
    " - Create Standartize as an option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azeghost/git/lmdb_new/LMDB_Datasets/.data\n"
     ]
    }
   ],
   "source": [
    "%cd .data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\r\n",
      "drwxr-xr-x 4 azeghost azeghost 4096 Sep  5 21:29 .\r\n",
      "drwxrwxr-x 5 azeghost azeghost 4096 Sep  5 21:28 ..\r\n",
      "drwxr-xr-x 2 azeghost azeghost 4096 Sep  5 21:28 _training\r\n",
      "drwxr-xr-x 2 azeghost azeghost 4096 Sep  5 21:29 _validation\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la ./MNIST_LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def read_many_lmdb(lmdb_dir, num_images):\n",
    "\n",
    "    images, labels = [], {}\n",
    "    env = lmdb.open(lmdb_dir, readonly=True) # Start a new read transaction\n",
    "    \n",
    "    with env.begin() as txn:\n",
    "        for image_id in range(num_images):\n",
    "            data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
    "            dataset = pickle.loads(data)\n",
    "            images.append(dataset.get_image())\n",
    "            \n",
    "            \n",
    "            labels_list = [attr for attr in dir(dataset) if not callable(getattr(dataset, attr)) and (not attr.startswith(\"__\")) and \n",
    "                           (not attr in ['image','channels',  'size'] )]\n",
    "\n",
    "            for label in labels_list:\n",
    "                _lab = {label: eval(f'dataset.{label}')}\n",
    "                labels = {**labels, **_lab}\n",
    "                \n",
    "    env.close()\n",
    "    return {'images': images, **labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-90c2f9fce369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_many_lmdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/MNIST_LMDB/_training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-4472c81d8044>\u001b[0m in \u001b[0;36mread_many_lmdb\u001b[0;34m(lmdb_dir, num_images)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{image_id:08}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "ds = read_many_lmdb('/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/MNIST_LMDB/_training', 60000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['images'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_LMDB/_training/\n",
      "MNIST_LMDB/_training/lock.mdb\n",
      "MNIST_LMDB/_training/data.mdb\n",
      "MNIST_LMDB/_validation/\n",
      "MNIST_LMDB/_validation/lock.mdb\n",
      "MNIST_LMDB/_validation/data.mdb\n",
      "-rw-r--r-- 1 azeghost azeghost 11M Sep  5 21:56 mnist.tar.partaa\n",
      "total 227772\n",
      "drwxrwxr-x 5 azeghost azeghost     4096 Sep  5 21:56 .\n",
      "drwxr-xr-x 8 azeghost azeghost     4096 Aug 11 15:43 ..\n",
      "drwxr-xr-x 4 azeghost azeghost     4096 Sep  5 21:29 MNIST_LMDB\n",
      "-rw-r--r-- 1 azeghost azeghost 11490434 Sep  5 18:30 mnist.npz\n",
      "-rw-r--r-- 1 azeghost azeghost 11196828 Sep  5 21:56 mnist.tar.bz\n",
      "-rw-r--r-- 1 azeghost azeghost 11196828 Sep  5 21:56 mnist.tar.partaa\n",
      "-rw-r--r-- 1 azeghost azeghost 66439133 Sep  5 17:30 pokemon_combined.tar.bz\n",
      "drwxr-xr-x 4 azeghost azeghost     4096 Aug  2 20:18 Pokemon_LMDB\n",
      "-rw-r--r-- 1 azeghost azeghost 66439133 Sep  5 17:21 pokemon.tar.bz\n",
      "-rw-r--r-- 1 azeghost azeghost 52428800 Sep  5 17:21 pokemon.tar.partaa\n",
      "-rw-r--r-- 1 azeghost azeghost 14010333 Sep  5 17:21 pokemon.tar.partab\n",
      "drwxr-xr-x 3 azeghost azeghost     4096 Sep  5 17:49 .test\n",
      "total 238708\n",
      "drwxrwxr-x 5 azeghost azeghost     4096 Sep  5 21:56 .\n",
      "drwxr-xr-x 8 azeghost azeghost     4096 Aug 11 15:43 ..\n",
      "-rw-r--r-- 1 azeghost azeghost 11196828 Sep  5 21:56 mnist_combined.tar.bz\n",
      "drwxr-xr-x 4 azeghost azeghost     4096 Sep  5 21:29 MNIST_LMDB\n",
      "-rw-r--r-- 1 azeghost azeghost 11490434 Sep  5 18:30 mnist.npz\n",
      "-rw-r--r-- 1 azeghost azeghost 11196828 Sep  5 21:56 mnist.tar.bz\n",
      "-rw-r--r-- 1 azeghost azeghost 11196828 Sep  5 21:56 mnist.tar.partaa\n",
      "-rw-r--r-- 1 azeghost azeghost 66439133 Sep  5 17:30 pokemon_combined.tar.bz\n",
      "drwxr-xr-x 4 azeghost azeghost     4096 Aug  2 20:18 Pokemon_LMDB\n",
      "-rw-r--r-- 1 azeghost azeghost 66439133 Sep  5 17:21 pokemon.tar.bz\n",
      "-rw-r--r-- 1 azeghost azeghost 52428800 Sep  5 17:21 pokemon.tar.partaa\n",
      "-rw-r--r-- 1 azeghost azeghost 14010333 Sep  5 17:21 pokemon.tar.partab\n",
      "drwxr-xr-x 3 azeghost azeghost     4096 Sep  5 17:49 .test\n",
      "/home/azeghost/git/lmdb_new/LMDB_Datasets/.data\n",
      "mkdir: cannot create directory ‘./.test’: File exists\n",
      "total 16\n",
      "drwxr-xr-x 4 azeghost azeghost 4096 Sep  5 17:49 .\n",
      "drwxr-xr-x 4 azeghost azeghost 4096 Sep  5 21:56 ..\n",
      "drwxr-xr-x 2 azeghost azeghost 4096 Aug  2 20:18 _training\n",
      "drwxr-xr-x 2 azeghost azeghost 4096 Aug  2 20:18 _validation\n"
     ]
    }
   ],
   "source": [
    "!tar -cvjf mnist.tar.bz MNIST_LMDB/* \n",
    "\n",
    "!split -b 50M mnist.tar.bz \"mnist.tar.part\"\n",
    "#split -b <max size> <name of zip or dir to zip/name> <split file name beginning>\n",
    "\n",
    "#Check if they are created\n",
    "!ls -lh mnist.tar.part*\n",
    "\n",
    "import os\n",
    "os.chdir('/home/azeghost/git/lmdb_new/LMDB_Datasets/.data')\n",
    "\n",
    "!ls -la\n",
    "\n",
    "#move to correct folder and push to git\n",
    "# !mkdir /home/azeghost/git/LMDB_Datasets/pokemon\n",
    "# !mv pokemon.tar.part* /home/azeghost/git/Generative_Models/data/.pokemon\n",
    "# !ls -la /home/azeghost/git/Generative_Models/data/.pokemon \n",
    "\n",
    "# Combine Them\n",
    "\n",
    "#pokemon_training\n",
    "!cat mnist.tar.part* > mnist_combined.tar.bz\n",
    "#!cat <split files put * at the end> > <final zip name>\n",
    "!ls -la \n",
    "!pwd\n",
    "\n",
    "!mkdir ./.test\n",
    "\n",
    "!tar -xf mnist_combined.tar.bz --directory ./.test\n",
    "\n",
    "!ls -la ./.test/Pokemon_LMDB\n",
    "\n",
    "# !rm -rf ./.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMDBImageIterator and LMDBImageGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/azeghost/git/Generative_Models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import Iterator, load_img, img_to_array, array_to_img\n",
    "from keras import backend as K\n",
    "import logging\n",
    "from utils.reporting.logging import log_message\n",
    "from utils.data_and_files.file_utils import get_file_path\n",
    "\n",
    "class LMDBImageIterator(Iterator):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_images,\n",
    "                 category,\n",
    "                 lmdb_dir,\n",
    "                 batch_size,\n",
    "                 episode_len=20,\n",
    "                 episode_shift=10,\n",
    "                 shuffle=True,\n",
    "                 seed=None,\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='jpeg',\n",
    "                 dtype=K.floatx(),\n",
    "                 ):\n",
    "        \n",
    "        self.category = category\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        self.lmdb_dir = lmdb_dir\n",
    "        self.episode_len = episode_len\n",
    "        self.episode_shift = episode_shift\n",
    "\n",
    "        \n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "        print(\"Initializing Iterator \" + category +\" Number of images \" +str(num_images))\n",
    "        print(category,lmdb_dir, batch_size,shuffle,seed)\n",
    "        self.env = lmdb.open(lmdb_dir, readonly=True)\n",
    "        \n",
    "        Iterator.__init__(self, num_images, batch_size, shuffle, seed)\n",
    "        \n",
    "        \n",
    "    def __del__(self):\n",
    "        self.env.close()\n",
    "        \n",
    "        \n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        print(index_array)\n",
    "        images, labels = [], []\n",
    "        \n",
    "        if len(index_array) < self.batch_size:\n",
    "            diff = self.batch_size//len(index_array) + 1\n",
    "            index_array = np.repeat(index_array, diff, axis=0)[:self.batch_size]\n",
    "\n",
    "        else:\n",
    "                with self.env.begin() as txn:\n",
    "                    for image_id in index_array:\n",
    "                        data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
    "                        dataset = pickle.loads(data)\n",
    "                        images.append(dataset.get_image())\n",
    "                        labels_list = [attr for attr in dir(dataset) if not callable(getattr(dataset, attr)) and (not attr.startswith(\"__\")) and \n",
    "                           (not attr in ['image','channels',  'size'] )]\n",
    "\n",
    "                        for label in labels_list:\n",
    "                            _lab = {label: eval(f'dataset.{label}')}\n",
    "                            labels = {**labels, **_lab}\n",
    "        return {'images': images, **labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from utils.data_and_files.data_utils import as_bytes\n",
    "from utils.reporting.logging import log_message\n",
    "\n",
    "\n",
    "class LMDBImageGenerator(ImageDataGenerator):\n",
    "    def flow_from_lmdb_lists(self, \n",
    "                              num_images,\n",
    "                              category,\n",
    "                              lmdb_dir,\n",
    "                              batch_size,\n",
    "                              episode_len=None,\n",
    "                              episode_shift=None,\n",
    "                              color_mode='rgb',\n",
    "                              shuffle =True,\n",
    "                              seed=None\n",
    "                              ):\n",
    "\n",
    "        \n",
    "          \n",
    "\n",
    "        return LMDBImageIterator(\n",
    "                             num_images = num_images,\n",
    "                             category = category,\n",
    "                             lmdb_dir = lmdb_dir,\n",
    "                             batch_size  = batch_size,\n",
    "                             episode_len = episode_len,\n",
    "                             episode_shift =episode_shift,\n",
    "                             shuffle = shuffle,\n",
    "                             seed = seed)\n",
    "\n",
    "\n",
    "def get_generators( val_lmdb_dir, val_num_images, tra_lmdb_dir, tra_num_images, \n",
    "                   batch_size, episode_len=None, episode_shift=None):\n",
    "\n",
    "    train_datagen = LMDBImageGenerator()\n",
    "\n",
    "    valid_datagen = LMDBImageGenerator()\n",
    "\n",
    "    train_generator = train_datagen.flow_from_lmdb_lists(\n",
    "        num_images = tra_num_images,\n",
    "        category='training',\n",
    "        lmdb_dir=tra_lmdb_dir,\n",
    "        batch_size=batch_size,\n",
    "        episode_len=episode_len,\n",
    "        episode_shift=episode_shift,\n",
    "        seed=0)\n",
    "\n",
    "    validation_generator = valid_datagen.flow_from_lmdb_lists(\n",
    "        num_images = val_num_images,\n",
    "        category='validation',\n",
    "        lmdb_dir=val_lmdb_dir,\n",
    "        batch_size=batch_size,\n",
    "        episode_len=episode_len,\n",
    "        episode_shift=episode_shift,\n",
    "        seed=0)\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator, testing_generator = get_generators(\n",
    "    val_lmdb_dir = '/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/Pokemon_LMDB/_validation',\n",
    "    val_num_images = 218,\n",
    "    tra_lmdb_dir = '/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/Pokemon_LMDB/_training', \n",
    "    tra_num_images = 591,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y = training_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: training_generator, \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: training_generator, \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: training_generator, \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: testing_generator,     \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")\n",
    "\n",
    "_instance_scale=1.0\n",
    "for data in train_ds:\n",
    "    _instance_scale = float(data[0].numpy().max())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
