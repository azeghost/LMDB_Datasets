{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1240512\r\n",
      "drwxrwxr-x 2 azeghost azeghost      4096 Oct 14 20:07 .\r\n",
      "drwxr-xr-x 4 azeghost azeghost      4096 Oct 14 20:07 ..\r\n",
      "-rw-rw-r-- 1 azeghost azeghost 254052267 Nov 15  2019 mini-imagenet-cache-test.pkl\r\n",
      "-rw-rw-r-- 1 azeghost azeghost 812967380 Nov 15  2019 mini-imagenet-cache-train.pkl\r\n",
      "-rw-rw-r-- 1 azeghost azeghost 203241803 Nov 15  2019 mini-imagenet-cache-val.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la /home/azeghost/datasets/MiniImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import lmdb\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azeghost/git/lmdb_new/LMDB_Datasets\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../') # adress to git dir\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir .data/imagenet_LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf .data/imagenet_LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls .data/imagenet_LMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image_data', 'class_dict'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-imagenet-cache-test.pkl\n",
    "\n",
    "train_in = open(\"/home/azeghost/datasets/MiniImageNet/mini-imagenet-cache-train.pkl\", \"rb\")\n",
    "\n",
    "train = pickle.load(train_in)\n",
    "\n",
    "Xtrain = train[\"image_data\"]\n",
    "\n",
    "Xtrain = Xtrain.reshape([-1, 84, 84, 3])\n",
    "# 64 * 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Wrapper class for dataset\n",
    "class DatasetWrapper:\n",
    "    def __init__(self, image):\n",
    "        try:\n",
    "            self.channels = image.shape[2]\n",
    "        except:\n",
    "            self.channels = 1\n",
    "        self.size = image.shape[:2]\n",
    "        self.image = image.tobytes()\n",
    "\n",
    "    def get_image(self):\n",
    "        \"\"\" Returns the image as a numpy array. \"\"\"\n",
    "        images = np.frombuffer(self.image, dtype=np.float32) #pay attention if you  don't use create_image_lists\n",
    "        return images.reshape(*self.size, self.channels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb \n",
    "def store_single_lmdb( filename, img, index,  num_images):\n",
    "    \"\"\" Stores a wrapper to LMDB.\n",
    "    \"\"\"\n",
    "    map_size = num_images * img.nbytes * 10\n",
    "    env = lmdb.open(filename, map_size=map_size)\n",
    "\n",
    "    # Same as before — but let's write all the images in a single transaction\n",
    "    with env.begin(write=True) as txn:\n",
    "        # All key-value pairs need to be Strings\n",
    "        value = DatasetWrapper(img)\n",
    "        key = f\"{index:08}\"\n",
    "        txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'train'\n",
    "scaler = 255.0\n",
    "images = Xtrain\n",
    "lmdb_dir = \".data/imagenet_LMDB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38400/38400 [01:59<00:00, 320.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# create_if_not_exist(lmdb_dir)\n",
    "num_images = images.shape[0]\n",
    "lmdb_name = lmdb_dir + os.sep + '_{}'.format(category)\n",
    "index = 0\n",
    "for idx, (image) in tqdm(enumerate(images),total=num_images):\n",
    "                img = np.float32(image) / scaler\n",
    "\n",
    "                store_single_lmdb(index=index, filename=lmdb_name, img=img,\n",
    "                                       num_images=num_images)\n",
    "                index = index + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Read the lmdb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Todo\n",
    " - Env put outside of loops for write and read\n",
    " - Try the Dynamic code again with clean kernel \n",
    " - Create Standartize as an option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lmdb(lmdb_dir, num_images):\n",
    "    images = []\n",
    "    env = lmdb.open(lmdb_dir, readonly=True)\n",
    "\n",
    "    # Start a new read transaction\n",
    "    with env.begin() as txn:\n",
    "        # Read all images in one single transaction, with one lock\n",
    "        # We could split this up into multiple transactions if needed\n",
    "        for image_id in range(num_images):\n",
    "            data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
    "\n",
    "            dataset = pickle.loads(data)\n",
    "            images.append(dataset.get_image())\n",
    "            labels_list = [attr for attr in dir(dataset) if\n",
    "                           not callable(getattr(dataset, attr)) and (not attr.startswith(\"__\")) and\n",
    "                           (not attr in ['image', 'channels', 'size'])]\n",
    "\n",
    "            for label in labels_list:\n",
    "                # _lab = {label: eval(f'dataset.{label}')}\n",
    "                # labels = {**labels, **_lab}\n",
    "                if label in labels:\n",
    "                    labels[label].append(eval(f'dataset.{label}'))\n",
    "                else:\n",
    "                    labels = {label: [eval(f'dataset.{label}')] }\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    return {'images': images, **labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tar and unzip LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/azeghost/git/lmdb_new/LMDB_Datasets/.data')\n",
    "# !ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tar -cvjf mnist.tar.bz MNIST_LMDB/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv mnist.tar.bz /home/azeghost/git/lmdb_new/LMDB_Datasets/.data/MNIST_TAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/azeghost/git/lmdb_new/LMDB_Datasets/.data/MNIST_TAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !split -b 50M mnist.tar.bz \"mnist.tar.part\"\n",
    "#split -b <max size> <name of zip or dir to zip/name> <split file name beginning>\n",
    "\n",
    "#Check if they are created\n",
    "# !ls -lh mnist.tar.part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine Them\n",
    "\n",
    "#pokemon_training\n",
    "# !cat mnist.tar.part* > mnist_combined.tar.bz\n",
    "#!cat <split files put * at the end> > <final zip name>\n",
    "# !ls -la \n",
    "\n",
    "!mkdir ./.test_Mnist\n",
    "\n",
    "!tar -xf mnist.tar.bz --directory ./.test_Mnist\n",
    "\n",
    "# !rm -rf ./.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ./.test_Mnist/MNIST_LMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMDBImageIterator and LMDBImageGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/azeghost/git/Generative_Models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import Iterator, load_img, img_to_array, array_to_img\n",
    "from keras import backend as K\n",
    "import logging\n",
    "from utils.reporting.logging import log_message\n",
    "from utils.data_and_files.file_utils import get_file_path\n",
    "\n",
    "class LMDBImageIterator(Iterator):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_images,\n",
    "                 category,\n",
    "                 lmdb_dir,\n",
    "                 batch_size,\n",
    "                 episode_len=20,\n",
    "                 episode_shift=10,\n",
    "                 shuffle=True,\n",
    "                 seed=None,\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='jpeg',\n",
    "                 dtype=K.floatx(),\n",
    "                 ):\n",
    "        \n",
    "        self.category = category\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        self.lmdb_dir = lmdb_dir\n",
    "        self.episode_len = episode_len\n",
    "        self.episode_shift = episode_shift\n",
    "\n",
    "        \n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "        print(\"Initializing Iterator \" + category +\" Number of images \" +str(num_images))\n",
    "        print(category,lmdb_dir, batch_size,shuffle,seed)\n",
    "        self.env = lmdb.open(lmdb_dir, readonly=True)\n",
    "        \n",
    "        Iterator.__init__(self, num_images, batch_size, shuffle, seed)\n",
    "        \n",
    "        \n",
    "    def __del__(self):\n",
    "        self.env.close()\n",
    "        \n",
    "        \n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        print(index_array)\n",
    "        images, labels = [], []\n",
    "        \n",
    "        if len(index_array) < self.batch_size:\n",
    "            diff = self.batch_size//len(index_array) + 1\n",
    "            index_array = np.repeat(index_array, diff, axis=0)[:self.batch_size]\n",
    "\n",
    "        else:\n",
    "                with self.env.begin() as txn:\n",
    "                    for image_id in index_array:\n",
    "                        data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
    "                        dataset = pickle.loads(data)\n",
    "                        images.append(dataset.get_image())\n",
    "                        labels_list = [attr for attr in dir(dataset) if not callable(getattr(dataset, attr)) and (not attr.startswith(\"__\")) and \n",
    "                           (not attr in ['image','channels',  'size'] )]\n",
    "\n",
    "                        for label in labels_list:\n",
    "                            _lab = {label: eval(f'dataset.{label}')}\n",
    "                            labels = {**labels, **_lab}\n",
    "        return {'images': images, **labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from utils.data_and_files.data_utils import as_bytes\n",
    "from utils.reporting.logging import log_message\n",
    "\n",
    "\n",
    "class LMDBImageGenerator(ImageDataGenerator):\n",
    "    def flow_from_lmdb_lists(self, \n",
    "                              num_images,\n",
    "                              category,\n",
    "                              lmdb_dir,\n",
    "                              batch_size,\n",
    "                              episode_len=None,\n",
    "                              episode_shift=None,\n",
    "                              color_mode='rgb',\n",
    "                              shuffle =True,\n",
    "                              seed=None\n",
    "                              ):\n",
    "\n",
    "        \n",
    "          \n",
    "\n",
    "        return LMDBImageIterator(\n",
    "                             num_images = num_images,\n",
    "                             category = category,\n",
    "                             lmdb_dir = lmdb_dir,\n",
    "                             batch_size  = batch_size,\n",
    "                             episode_len = episode_len,\n",
    "                             episode_shift =episode_shift,\n",
    "                             shuffle = shuffle,\n",
    "                             seed = seed)\n",
    "\n",
    "\n",
    "def get_generators( val_lmdb_dir, val_num_images, tra_lmdb_dir, tra_num_images, \n",
    "                   batch_size, episode_len=None, episode_shift=None):\n",
    "\n",
    "    train_datagen = LMDBImageGenerator()\n",
    "\n",
    "    valid_datagen = LMDBImageGenerator()\n",
    "\n",
    "    train_generator = train_datagen.flow_from_lmdb_lists(\n",
    "        num_images = tra_num_images,\n",
    "        category='training',\n",
    "        lmdb_dir=tra_lmdb_dir,\n",
    "        batch_size=batch_size,\n",
    "        episode_len=episode_len,\n",
    "        episode_shift=episode_shift,\n",
    "        seed=0)\n",
    "\n",
    "    validation_generator = valid_datagen.flow_from_lmdb_lists(\n",
    "        num_images = val_num_images,\n",
    "        category='validation',\n",
    "        lmdb_dir=val_lmdb_dir,\n",
    "        batch_size=batch_size,\n",
    "        episode_len=episode_len,\n",
    "        episode_shift=episode_shift,\n",
    "        seed=0)\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator, testing_generator = get_generators(\n",
    "    val_lmdb_dir = '/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/Pokemon_LMDB/_validation',\n",
    "    val_num_images = 218,\n",
    "    tra_lmdb_dir = '/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/Pokemon_LMDB/_training', \n",
    "    tra_num_images = 591,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y = training_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: training_generator, \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: training_generator, \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: training_generator, \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: testing_generator,     \n",
    "    output_types=(tf.float32, tf.float32) ,\n",
    "    output_shapes=(tf.TensorShape((batch_size* EPIS_LEN, ) + image_size), \n",
    "                   tf.TensorShape((batch_size* EPIS_LEN, ) + image_size)\n",
    "                  )\n",
    ")\n",
    "\n",
    "_instance_scale=1.0\n",
    "for data in train_ds:\n",
    "    _instance_scale = float(data[0].numpy().max())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
